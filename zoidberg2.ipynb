{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.9.0\n",
    "# pip install pandas\n",
    "# pip install -U scikit-learn\n",
    "# pip install pyyaml h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des dépendances requises pour le projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afficher la version de TensorFlow\n",
    "Cette ligne de code affiche la version actuelle de TensorFlow qui est utilisée dans l'environnement Python. Cela peut être utile pour confirmer que la bonne version de TensorFlow est installée ou pour s'assurer que le code fonctionnera sur d'autres machines avec la même version de TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# import data handling tools\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques et des outils nécessaires\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques système telles que os, time, shutil, pathlib et itertools.\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques pour le traitement de données telles que cv2, numpy, pandas, seaborn et matplotlib.pyplot. Nous utilisons également des outils de scikit-learn tels que train_test_split, confusion_matrix et classification_report.\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques de Deep Learning telles que tensorflow et keras. Nous utilisons également des couches spécifiques de Keras telles que Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout et BatchNormalization. Nous utilisons également des optimiseurs tels qu'Adam et Adamax.\n",
    "\n",
    "- Dans cette section, nous ignorons les avertissements pour rendre la sortie du code plus propre et lisible.\n",
    "\n",
    "- Enfin, nous affichons un message de confirmation pour indiquer que toutes les bibliothèques ont été importées avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "data_dir = 'chest_xray/train'\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "folds = os.listdir(data_dir)\n",
    "for fold in folds:\n",
    "    foldpath = os.path.join(data_dir, fold)\n",
    "    filelist = os.listdir(foldpath)\n",
    "    for file in filelist:\n",
    "        fpath = os.path.join(foldpath, file)\n",
    "        filepaths.append(fpath)\n",
    "        labels.append(fold)\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe\n",
    "Fseries = pd.Series(filepaths, name= 'filepaths')\n",
    "Lseries = pd.Series(labels, name='labels')\n",
    "df = pd.concat([Fseries, Lseries], axis= 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Générer des chemins de données avec des étiquettes\n",
    "Ce code lit les données d'un répertoire spécifié (dans ce cas, le dossier `chest_xray/train`), puis crée une liste de chemins de fichiers et d'étiquettes correspondantes. Il parcourt d'abord tous les dossiers du répertoire et ajoute tous les fichiers de chaque dossier à la liste de chemins de fichiers. Il utilise ensuite le nom du dossier comme étiquette pour chaque fichier correspondant. \n",
    "\n",
    "Enfin, les chemins de fichiers et les étiquettes sont combinés dans un dataframe pandas pour faciliter la manipulation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afficher le dataframe\n",
    "Cette partie du code affiche simplement le dataframe `df`, qui contient les chemins d'accès des images et leurs étiquettes correspondantes. Il peut être utile d'examiner ce dataframe pour s'assurer que les données sont correctement formatées et étiquetées. \n",
    "\n",
    "Pour afficher le dataframe, il suffit d'exécuter ce bloc de code en sélectionnant la cellule et en appuyant sur Shift+Enter ou en cliquant sur le bouton \"Exécuter\" dans la barre d'outils. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataframe\n",
    "train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123)\n",
    "\n",
    "# valid and test dataframe\n",
    "valid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "Cette partie du code utilise la fonction `train_test_split` de la bibliothèque scikit-learn pour diviser le dataframe `df` en ensembles d'entraînement, de validation et de test. \n",
    "\n",
    "Il divise d'abord `df` en deux ensembles: l'ensemble d'entraînement et un ensemble qui contient les données restantes. Ensuite, il divise l'ensemble de données restantes en deux ensembles: l'ensemble de validation et l'ensemble de test.\n",
    "\n",
    "La taille de chaque ensemble est contrôlée par les paramètres `train_size` et `test_size` de la fonction `train_test_split`, qui sont respectivement définis à 0,8 et 0,6 dans ce cas. Le paramètre `shuffle` contrôle si les données sont mélangées avant la division, et `random_state` fixe la graine pour la génération de nombres aléatoires, garantissant ainsi que la même division sera obtenue à chaque exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crobed image size\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "# Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "def scalar(img):\n",
    "    return img\n",
    "\n",
    "tr_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "\n",
    "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "# Note: we will use custom test_batch_size, and make shuffle= false\n",
    "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de générateurs de données d'images pour l'entraînement, la validation et le test.\n",
    "\n",
    "- Dans un premier temps, nous définition des tailles d'images.\n",
    "\n",
    "- Détermination de la taille de lot recommandée pour les données de test, à l'aide d'une fonction personnalisée.\n",
    "Cette fonction calcule la taille optimale pour chaque lot de données, en divisant le nombre d'échantillons de test\n",
    "par un nombre entier jusqu'à 80. La taille maximale est choisie pour minimiser les calculs tout en utilisant\n",
    "la mémoire disponible.\n",
    "\n",
    "- Définition d'une fonction qui sera utilisée pour effectuer l'augmentation des données de l'ensemble d'entraînement.\n",
    "Cette fonction prend simplement l'image en entrée et la renvoie telle quelle.\n",
    "\n",
    "- Création des générateurs d'images pour l'ensemble d'entraînement et l'ensemble de validation.\n",
    "\n",
    "- Création du générateur d'images pour l'ensemble de test.\n",
    "Note: nous allons utiliser une taille de lot personnalisée pour les données de test, et shuffle=False pour ne pas\n",
    "perturber l'ordre des images dans les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
    "classes = list(g_dict.keys())       # defines list of dictionary's kays (classes), classes names : string\n",
    "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    image = images[i] / 255       # scales data to range (0 - 255)\n",
    "    plt.imshow(image)\n",
    "    index = np.argmax(labels[i])  # get image index\n",
    "    class_name = classes[index]   # get class of image\n",
    "    plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage d'un lot d'images avec leurs étiquettes\n",
    "\n",
    "Ce code affiche un lot de 16 images à partir d'un générateur de données train_gen. Il utilise un dictionnaire class_indices pour mapper chaque nom de classe à son indice correspondant. Ensuite, il récupère une liste de noms de classe à partir des clés du dictionnaire.\n",
    "\n",
    "Le code utilise ensuite la fonction next() pour extraire un lot de données et d'étiquettes à partir du générateur. Chaque image est normalisée en divisant ses valeurs de pixel par 255 pour les mettre à l'échelle dans la plage (0 - 1).\n",
    "\n",
    "Enfin, le code affiche chaque image avec son étiquette correspondante dans un sous-ensemble de la figure matplotlib, en utilisant le nom de la classe pour étiqueter chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "        Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                    bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "        Dropout(rate= 0.45, seed= 123),\n",
    "        Dense(class_count, activation= 'softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la structure du modèle et définition de sa compilation\n",
    "\n",
    "La fonction create_model() crée le modèle en utilisant les couches mentionnées ci-dessus et définit les paramètres de compilation. Le modèle est ensuite compilé avec l'optimiseur Adamax, une fonction de coût categorical_crossentropy et une métrique d'évaluation de précision.\n",
    "\n",
    "La variable class_count est définie à partir de la liste des indices de classes fournies par le générateur de données train_gen pour déterminer le nombre de classes dans la couche de sortie. Le modèle est ensuite instancié en appelant create_model() et son architecture est affichée en utilisant la méthode summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=5*batch_size)\n",
    "\n",
    "epochs = 10 # number of all epochs in training\n",
    "\n",
    "history = model.fit(x= train_gen,\n",
    "                    epochs= epochs,\n",
    "                    verbose= 1,\n",
    "                    validation_data= valid_gen, \n",
    "                    validation_steps= None,\n",
    "                    shuffle= False,\n",
    "                    callbacks=[cp_callback])  # Pass callback to training  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrer les poids du modèle à intervalles réguliers pendant l'entraînement\n",
    "\n",
    "Ce code utilise la fonction tf.keras.callbacks.ModelCheckpoint pour enregistrer les poids du modèle à intervalles réguliers pendant l'entraînement. Il prend en entrée plusieurs paramètres pour personnaliser le comportement du rappel de contrôle, tels que le chemin d'accès au fichier de point de contrôle, la fréquence d'enregistrement des points de contrôle, etc.\n",
    "\n",
    "Le code crée également une instance du rappel de contrôle et la passe au modèle lors de l'appel à la méthode fit() pour l'entraînement. De cette façon, le modèle enregistrera automatiquement les poids à chaque intervalle spécifié par le rappel de contrôle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrer les poids du modèle\n",
    "\n",
    "Ce code utilise la méthode save_weights() pour enregistrer les poids du modèle dans un fichier sur le disque. Le chemin d'accès et le nom du fichier de point de contrôle sont spécifiés dans la chaîne passée à la méthode save_weights()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# end the training\n",
    "\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(train_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger des poids précédemment sauvegardés\n",
    "\n",
    "Ce code crée une nouvelle instance de modèle à partir de la fonction create_model() et utilise tf.train.latest_checkpoint() pour trouver le dernier point de contrôle sauvegardé dans le répertoire spécifié. Ensuite, le modèle charge les poids précédemment sauvegardés à l'aide de model.load_weights()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/my_model1') # downgrade to 2.9.1 tf version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, ce code enregistre le modèle actuel en utilisant la fonction model.save() avec un nom de fichier spécifié. Dans cet exemple, le nom de fichier est 'saved_model/my_model1'. Cela permet de sauvegarder le modèle et ses poids dans un format standard pour les modèles TensorFlow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
