{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.9.0\n",
    "# pip install pandas\n",
    "# pip install -U scikit-learn\n",
    "# pip install pyyaml h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des dépendances requises pour le projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afficher la version de TensorFlow\n",
    "Cette ligne de code affiche la version actuelle de TensorFlow qui est utilisée dans l'environnement Python. Cela peut être utile pour confirmer que la bonne version de TensorFlow est installée ou pour s'assurer que le code fonctionnera sur d'autres machines avec la même version de TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques et des outils nécessaires\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques système telles que os, time, shutil, pathlib et itertools.\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques pour le traitement de données telles que cv2, numpy, pandas, seaborn et matplotlib.pyplot. Nous utilisons également des outils de scikit-learn tels que train_test_split, confusion_matrix et classification_report.\n",
    "\n",
    "- Dans cette section, nous importons des bibliothèques de Deep Learning telles que tensorflow et keras. Nous utilisons également des couches spécifiques de Keras telles que Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout et BatchNormalization. Nous utilisons également des optimiseurs tels qu'Adam et Adamax.\n",
    "\n",
    "- Dans cette section, nous ignorons les avertissements pour rendre la sortie du code plus propre et lisible.\n",
    "\n",
    "- Enfin, nous affichons un message de confirmation pour indiquer que toutes les bibliothèques ont été importées avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "data_dir = 'chest_xray/train'\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "folds = os.listdir(data_dir)\n",
    "for fold in folds:\n",
    "    foldpath = os.path.join(data_dir, fold)\n",
    "    filelist = os.listdir(foldpath)\n",
    "    for file in filelist:\n",
    "        fpath = os.path.join(foldpath, file)\n",
    "        filepaths.append(fpath)\n",
    "        labels.append(fold)\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe\n",
    "Fseries = pd.Series(filepaths, name= 'filepaths')\n",
    "Lseries = pd.Series(labels, name='labels')\n",
    "df = pd.concat([Fseries, Lseries], axis= 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Générer des chemins de données avec des étiquettes\n",
    "Ce code lit les données d'un répertoire spécifié (dans ce cas, le dossier `chest_xray/train`), puis crée une liste de chemins de fichiers et d'étiquettes correspondantes. Il parcourt d'abord tous les dossiers du répertoire et ajoute tous les fichiers de chaque dossier à la liste de chemins de fichiers. Il utilise ensuite le nom du dossier comme étiquette pour chaque fichier correspondant. \n",
    "\n",
    "Enfin, les chemins de fichiers et les étiquettes sont combinés dans un dataframe pandas pour faciliter la manipulation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afficher le dataframe\n",
    "Cette partie du code affiche simplement le dataframe `df`, qui contient les chemins d'accès des images et leurs étiquettes correspondantes. Il peut être utile d'examiner ce dataframe pour s'assurer que les données sont correctement formatées et étiquetées. \n",
    "\n",
    "Pour afficher le dataframe, il suffit d'exécuter ce bloc de code en sélectionnant la cellule et en appuyant sur Shift+Enter ou en cliquant sur le bouton \"Exécuter\" dans la barre d'outils. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataframe\n",
    "train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123)\n",
    "\n",
    "# valid and test dataframe\n",
    "valid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "Cette partie du code utilise la fonction `train_test_split` de la bibliothèque scikit-learn pour diviser le dataframe `df` en ensembles d'entraînement, de validation et de test. \n",
    "\n",
    "Il divise d'abord `df` en deux ensembles: l'ensemble d'entraînement et un ensemble qui contient les données restantes. Ensuite, il divise l'ensemble de données restantes en deux ensembles: l'ensemble de validation et l'ensemble de test.\n",
    "\n",
    "La taille de chaque ensemble est contrôlée par les paramètres `train_size` et `test_size` de la fonction `train_test_split`, qui sont respectivement définis à 0,8 et 0,6 dans ce cas. Le paramètre `shuffle` contrôle si les données sont mélangées avant la division, et `random_state` fixe la graine pour la génération de nombres aléatoires, garantissant ainsi que la même division sera obtenue à chaque exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crobed image size\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "# Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "def scalar(img):\n",
    "    return img\n",
    "\n",
    "tr_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "\n",
    "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "# Note: we will use custom test_batch_size, and make shuffle= false\n",
    "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de générateurs de données d'images pour l'entraînement, la validation et le test.\n",
    "\n",
    "- Dans un premier temps, nous définition des tailles d'images.\n",
    "\n",
    "- Détermination de la taille de lot recommandée pour les données de test, à l'aide d'une fonction personnalisée.\n",
    "Cette fonction calcule la taille optimale pour chaque lot de données, en divisant le nombre d'échantillons de test\n",
    "par un nombre entier jusqu'à 80. La taille maximale est choisie pour minimiser les calculs tout en utilisant\n",
    "la mémoire disponible.\n",
    "\n",
    "- Définition d'une fonction qui sera utilisée pour effectuer l'augmentation des données de l'ensemble d'entraînement.\n",
    "Cette fonction prend simplement l'image en entrée et la renvoie telle quelle.\n",
    "\n",
    "- Création des générateurs d'images pour l'ensemble d'entraînement et l'ensemble de validation.\n",
    "\n",
    "- Création du générateur d'images pour l'ensemble de test.\n",
    "Note: nous allons utiliser une taille de lot personnalisée pour les données de test, et shuffle=False pour ne pas\n",
    "perturber l'ordre des images dans les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
    "classes = list(g_dict.keys())       # defines list of dictionary's kays (classes), classes names : string\n",
    "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    image = images[i] / 255       # scales data to range (0 - 255)\n",
    "    plt.imshow(image)\n",
    "    index = np.argmax(labels[i])  # get image index\n",
    "    class_name = classes[index]   # get class of image\n",
    "    plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "        Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                    bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "        Dropout(rate= 0.45, seed= 123),\n",
    "        Dense(class_count, activation= 'softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=5*batch_size)\n",
    "\n",
    "epochs = 10 # number of all epochs in training\n",
    "\n",
    "history = model.fit(x= train_gen,\n",
    "                    epochs= epochs,\n",
    "                    verbose= 1,\n",
    "                    validation_data= valid_gen, \n",
    "                    validation_steps= None,\n",
    "                    shuffle= False,\n",
    "                    callbacks=[cp_callback])  # Pass callback to training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# end the training\n",
    "\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(train_gen, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/my_model1') # downgrade to 2.9.1 tf version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
